{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to Generate dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at: C:\\Users\\rushi\\Desktop\\Data set\\dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = r\"C:\\Users\\rushi\\Desktop\\Data set\"\n",
    "\n",
    "# Define categories (Deepfake folders = label 1, Original = label 0)\n",
    "categories = {\n",
    "    \"Deepfakes\": 1,\n",
    "    \"Face2Face\": 1,\n",
    "    \"FaceShifter\": 1,\n",
    "    \"FaceSwap\": 1,\n",
    "    \"NeuralTextures\": 1,\n",
    "    \"original\": 0\n",
    "}\n",
    "\n",
    "# Create a list to store file paths and labels\n",
    "data = []\n",
    "\n",
    "# Loop through each category folder\n",
    "for category, label in categories.items():\n",
    "    category_path = os.path.join(DATASET_PATH, category)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(category_path):\n",
    "        print(f\"Warning: Folder not found -> {category_path}\")\n",
    "        continue\n",
    "\n",
    "    # Loop through video files in the folder\n",
    "    for video_name in os.listdir(category_path):\n",
    "        video_path = os.path.join(category_path, video_name)\n",
    "        \n",
    "        # Check if it's a valid file\n",
    "        if os.path.isfile(video_path):\n",
    "            data.append([video_path, label])\n",
    "\n",
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"video_path\", \"label\"])\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = os.path.join(DATASET_PATH, \"dataset.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\000_003.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\001_870.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\002_006.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\003_000.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\004_982.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\005_010.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\006_002.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\007_132.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\008_990.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\009_027.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\010_005.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\011_805.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\012_026.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\013_883.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\014_790.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\015_919.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\016_209.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\017_803.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\018_019.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\019_018.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\020_344.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\021_312.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\022_489.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\023_923.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\024_073.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\025_067.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\026_012.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\027_009.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\028_068.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\029_048.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\030_193.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\031_163.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\032_944.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\033_097.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\034_590.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\035_036.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\036_035.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\037_072.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\038_125.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\039_058.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\040_997.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\041_063.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\042_084.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\043_110.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\044_945.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\045_889.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\046_904.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\047_862.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\048_029.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\049_946.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\050_059.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\051_332.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\052_108.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\053_095.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\054_071.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\055_147.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\056_996.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\057_070.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\058_039.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\059_050.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\060_088.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\061_080.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\062_066.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\063_041.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\064_991.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\065_089.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\066_062.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\067_025.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\068_028.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\069_961.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\070_057.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\071_054.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\072_037.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\073_024.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\074_825.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\075_977.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\076_079.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\077_100.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\078_955.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\079_076.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\080_061.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\081_087.mp4\n",
      "Extracted 10 frames from C:\\Users\\rushi\\Desktop\\Data set\\Deepfakes\\082_103.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_frames \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m FRAMES_PER_VIDEO)  \u001b[38;5;66;03m# Pick frames evenly\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(FRAMES_PER_VIDEO):\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCAP_PROP_POS_FRAMES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Jump to frame\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     success, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset CSV\n",
    "CSV_PATH = r\"C:\\Users\\rushi\\Desktop\\Data set\\dataset.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Folder to store extracted frames\n",
    "FRAME_SAVE_PATH = r\"C:\\Users\\rushi\\Desktop\\Data set\\frames\"\n",
    "os.makedirs(FRAME_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Number of frames to extract per video (adjust as needed)\n",
    "FRAMES_PER_VIDEO = 10\n",
    "\n",
    "# Process each video\n",
    "for index, row in df.iterrows():\n",
    "    video_path = row[\"video_path\"]\n",
    "    label = row[\"label\"]\n",
    "    \n",
    "    # Create label-specific folder\n",
    "    label_folder = os.path.join(FRAME_SAVE_PATH, str(label))\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_path}\")\n",
    "        continue\n",
    "\n",
    "    frame_count = 0\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    step = max(1, total_frames // FRAMES_PER_VIDEO)  # Pick frames evenly\n",
    "\n",
    "    for i in range(FRAMES_PER_VIDEO):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)  # Jump to frame\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        # Save frame as an image\n",
    "        frame_filename = f\"{os.path.basename(video_path).split('.')[0]}_frame{i}.jpg\"\n",
    "        frame_path = os.path.join(label_folder, frame_filename)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames from {video_path}\")\n",
    "\n",
    "print(\"✅ Frame extraction complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frames_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frames dataset saved at: C:/Users/rushi/Desktop/Data set/frames_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "FRAMES_DIR = \"C:/Users/rushi/Desktop/Data set/frames\"\n",
    "DATASET_CSV = \"C:/Users/rushi/Desktop/Data set/dataset.csv\"\n",
    "OUTPUT_CSV = \"C:/Users/rushi/Desktop/Data set/frames_dataset.csv\"\n",
    "\n",
    "# Load existing dataset (to get video labels)\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "\n",
    "# Dictionary to store video-to-label mapping\n",
    "video_labels = dict(zip(df[\"video_path\"].apply(os.path.basename), df[\"label\"]))\n",
    "\n",
    "# Prepare new dataset\n",
    "frame_data = []\n",
    "\n",
    "# Traverse frames directory\n",
    "for root, _, files in os.walk(FRAMES_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith((\".jpg\", \".png\")):  # Only process image files\n",
    "            video_name = os.path.basename(root)  # Extract video name\n",
    "            frame_path = os.path.join(root, file)  # Full frame path\n",
    "            \n",
    "            # Assign label based on video name\n",
    "            label = video_labels.get(video_name, \"unknown\")  \n",
    "            \n",
    "            frame_data.append([frame_path, label])\n",
    "\n",
    "# Convert to DataFrame\n",
    "frame_df = pd.DataFrame(frame_data, columns=[\"frame_path\", \"label\"])\n",
    "\n",
    "# Save to CSV\n",
    "frame_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"✅ Frames dataset saved at: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting frame into numpy array and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved batch 0 (1000 images)\n",
      "✅ Saved batch 1 (1000 images)\n",
      "✅ Saved batch 2 (1000 images)\n",
      "✅ Saved batch 3 (1000 images)\n",
      "✅ Saved batch 4 (1000 images)\n",
      "✅ Saved batch 5 (1000 images)\n",
      "✅ Saved batch 6 (1000 images)\n",
      "✅ Saved batch 7 (1000 images)\n",
      "✅ Saved batch 8 (1000 images)\n",
      "✅ Saved batch 9 (1000 images)\n",
      "✅ Saved batch 10 (1000 images)\n",
      "✅ Saved batch 11 (1000 images)\n",
      "✅ Saved batch 12 (1000 images)\n",
      "✅ Saved batch 13 (1000 images)\n",
      "✅ Saved batch 14 (1000 images)\n",
      "✅ Saved batch 15 (1000 images)\n",
      "✅ Saved batch 16 (1000 images)\n",
      "✅ Saved batch 17 (1000 images)\n",
      "✅ Saved batch 18 (1000 images)\n",
      "✅ Saved batch 19 (1000 images)\n",
      "✅ All images processed and saved as .npy files!\n",
      "✅ Saved batch 0 (100 images)\n",
      "✅ Saved batch 1 (100 images)\n",
      "✅ Saved batch 2 (100 images)\n",
      "✅ Saved batch 3 (100 images)\n",
      "✅ Saved batch 4 (100 images)\n",
      "✅ Saved batch 5 (100 images)\n",
      "✅ Saved batch 6 (100 images)\n",
      "✅ Saved batch 7 (100 images)\n",
      "✅ Saved batch 8 (100 images)\n",
      "✅ Saved batch 9 (100 images)\n",
      "✅ Saved batch 10 (100 images)\n",
      "✅ Saved batch 11 (100 images)\n",
      "✅ Saved batch 12 (100 images)\n",
      "✅ Saved batch 13 (100 images)\n",
      "✅ Saved batch 14 (100 images)\n",
      "✅ Saved batch 15 (100 images)\n",
      "✅ Saved batch 16 (100 images)\n",
      "✅ Saved batch 17 (100 images)\n",
      "✅ Saved batch 18 (100 images)\n",
      "✅ Saved batch 19 (100 images)\n",
      "✅ Saved batch 20 (100 images)\n",
      "✅ Saved batch 21 (100 images)\n",
      "✅ Saved batch 22 (100 images)\n",
      "✅ Saved batch 23 (100 images)\n",
      "✅ Saved batch 24 (100 images)\n",
      "✅ Saved batch 25 (100 images)\n",
      "✅ Saved batch 26 (100 images)\n",
      "✅ Saved batch 27 (100 images)\n",
      "✅ Saved batch 28 (100 images)\n",
      "✅ Saved batch 29 (100 images)\n",
      "✅ Saved batch 30 (100 images)\n",
      "✅ Saved batch 31 (100 images)\n",
      "✅ Saved batch 32 (100 images)\n",
      "✅ Saved batch 33 (100 images)\n",
      "✅ Saved batch 34 (100 images)\n",
      "✅ Saved batch 35 (100 images)\n",
      "✅ Saved batch 36 (100 images)\n",
      "✅ Saved batch 37 (100 images)\n",
      "✅ Saved batch 38 (100 images)\n",
      "✅ Saved batch 39 (100 images)\n",
      "✅ Saved batch 40 (100 images)\n",
      "✅ Saved batch 41 (100 images)\n",
      "✅ Saved batch 42 (100 images)\n",
      "✅ Saved batch 43 (100 images)\n",
      "✅ Saved batch 44 (100 images)\n",
      "✅ Saved batch 45 (100 images)\n",
      "✅ Saved batch 46 (100 images)\n",
      "✅ Saved batch 47 (100 images)\n",
      "✅ Saved batch 48 (100 images)\n",
      "✅ Saved batch 49 (100 images)\n",
      "✅ Saved batch 50 (100 images)\n",
      "✅ Saved batch 51 (100 images)\n",
      "✅ Saved batch 52 (100 images)\n",
      "✅ Saved batch 53 (100 images)\n",
      "✅ Saved batch 54 (100 images)\n",
      "✅ Saved batch 55 (100 images)\n",
      "✅ Saved batch 56 (100 images)\n",
      "✅ Saved batch 57 (100 images)\n",
      "✅ Saved batch 58 (100 images)\n",
      "✅ Saved batch 59 (100 images)\n",
      "✅ Saved batch 60 (100 images)\n",
      "✅ Saved batch 61 (100 images)\n",
      "✅ Saved batch 62 (100 images)\n",
      "✅ Saved batch 63 (100 images)\n",
      "✅ Saved batch 64 (100 images)\n",
      "✅ Saved batch 65 (100 images)\n",
      "✅ Saved batch 66 (100 images)\n",
      "✅ Saved batch 67 (100 images)\n",
      "✅ Saved batch 68 (100 images)\n",
      "✅ Saved batch 69 (100 images)\n",
      "✅ Saved batch 70 (100 images)\n",
      "✅ Saved batch 71 (100 images)\n",
      "✅ Saved batch 72 (100 images)\n",
      "✅ Saved batch 73 (100 images)\n",
      "✅ Saved batch 74 (100 images)\n",
      "✅ Saved batch 75 (100 images)\n",
      "✅ Saved batch 76 (100 images)\n",
      "✅ Saved batch 77 (100 images)\n",
      "✅ Saved batch 78 (100 images)\n",
      "✅ Saved batch 79 (100 images)\n",
      "✅ Saved batch 80 (100 images)\n",
      "✅ Saved batch 81 (100 images)\n",
      "✅ Saved batch 82 (100 images)\n",
      "✅ Saved batch 83 (100 images)\n",
      "✅ Saved batch 84 (100 images)\n",
      "✅ Saved batch 85 (100 images)\n",
      "✅ Saved batch 86 (100 images)\n",
      "✅ Saved batch 87 (100 images)\n",
      "✅ Saved batch 88 (100 images)\n",
      "✅ Saved batch 89 (100 images)\n",
      "✅ Saved batch 90 (100 images)\n",
      "✅ Saved batch 91 (100 images)\n",
      "✅ Saved batch 92 (100 images)\n",
      "✅ Saved batch 93 (100 images)\n",
      "✅ Saved batch 94 (100 images)\n",
      "✅ Saved batch 95 (100 images)\n",
      "✅ Saved batch 96 (100 images)\n",
      "✅ Saved batch 97 (100 images)\n",
      "✅ Saved batch 98 (100 images)\n",
      "✅ Saved batch 99 (100 images)\n",
      "✅ Saved batch 100 (100 images)\n",
      "✅ Saved batch 101 (100 images)\n",
      "✅ Saved batch 102 (100 images)\n",
      "✅ Saved batch 103 (100 images)\n",
      "✅ Saved batch 104 (100 images)\n",
      "✅ Saved batch 105 (100 images)\n",
      "✅ Saved batch 106 (100 images)\n",
      "✅ Saved batch 107 (100 images)\n",
      "✅ Saved batch 108 (100 images)\n",
      "✅ Saved batch 109 (100 images)\n",
      "✅ Saved batch 110 (100 images)\n",
      "✅ Saved batch 111 (100 images)\n",
      "✅ Saved batch 112 (100 images)\n",
      "✅ Saved batch 113 (100 images)\n",
      "✅ Saved batch 114 (100 images)\n",
      "✅ Saved batch 115 (100 images)\n",
      "✅ Saved batch 116 (100 images)\n",
      "✅ Saved batch 117 (100 images)\n",
      "✅ Saved batch 118 (100 images)\n",
      "✅ Saved batch 119 (100 images)\n",
      "✅ Saved batch 120 (100 images)\n",
      "✅ Saved batch 121 (100 images)\n",
      "✅ Saved batch 122 (100 images)\n",
      "✅ Saved batch 123 (100 images)\n",
      "✅ Saved batch 124 (100 images)\n",
      "✅ Saved batch 125 (100 images)\n",
      "✅ Saved batch 126 (100 images)\n",
      "✅ Saved batch 127 (100 images)\n",
      "✅ Saved batch 128 (100 images)\n",
      "✅ Saved batch 129 (100 images)\n",
      "✅ Saved batch 130 (100 images)\n",
      "✅ Saved batch 131 (100 images)\n",
      "✅ Saved batch 132 (100 images)\n",
      "✅ Saved batch 133 (100 images)\n",
      "✅ Saved batch 134 (100 images)\n",
      "✅ Saved batch 135 (100 images)\n",
      "✅ Saved batch 136 (100 images)\n",
      "✅ Saved batch 137 (100 images)\n",
      "✅ Saved batch 138 (100 images)\n",
      "✅ Saved batch 139 (100 images)\n",
      "✅ Saved batch 140 (100 images)\n",
      "✅ Saved batch 141 (100 images)\n",
      "✅ Saved batch 142 (100 images)\n",
      "✅ Saved batch 143 (100 images)\n",
      "✅ Saved batch 144 (100 images)\n",
      "✅ Saved batch 145 (100 images)\n",
      "✅ Saved batch 146 (100 images)\n",
      "✅ Saved batch 147 (100 images)\n",
      "✅ Saved batch 148 (100 images)\n",
      "✅ Saved batch 149 (100 images)\n",
      "✅ Saved batch 150 (100 images)\n",
      "✅ Saved batch 151 (100 images)\n",
      "✅ Saved batch 152 (100 images)\n",
      "✅ Saved batch 153 (100 images)\n",
      "✅ Saved batch 154 (100 images)\n",
      "✅ Saved batch 155 (100 images)\n",
      "✅ Saved batch 156 (100 images)\n",
      "✅ Saved batch 157 (100 images)\n",
      "✅ Saved batch 158 (100 images)\n",
      "✅ Saved batch 159 (100 images)\n",
      "✅ Saved batch 160 (100 images)\n",
      "✅ Saved batch 161 (100 images)\n",
      "✅ Saved batch 162 (100 images)\n",
      "✅ Saved batch 163 (100 images)\n",
      "✅ Saved batch 164 (100 images)\n",
      "✅ Saved batch 165 (100 images)\n",
      "✅ Saved batch 166 (100 images)\n",
      "✅ Saved batch 167 (100 images)\n",
      "✅ Saved batch 168 (100 images)\n",
      "✅ Saved batch 169 (100 images)\n",
      "✅ Saved batch 170 (100 images)\n",
      "✅ Saved batch 171 (100 images)\n",
      "✅ Saved batch 172 (100 images)\n",
      "✅ Saved batch 173 (100 images)\n",
      "✅ Saved batch 174 (100 images)\n",
      "✅ Saved batch 175 (100 images)\n",
      "✅ Saved batch 176 (100 images)\n",
      "✅ Saved batch 177 (100 images)\n",
      "✅ Saved batch 178 (100 images)\n",
      "✅ Saved batch 179 (100 images)\n",
      "✅ Saved batch 180 (100 images)\n",
      "✅ Saved batch 181 (100 images)\n",
      "✅ Saved batch 182 (100 images)\n",
      "✅ Saved batch 183 (100 images)\n",
      "✅ Saved batch 184 (100 images)\n",
      "✅ Saved batch 185 (100 images)\n",
      "✅ Saved batch 186 (100 images)\n",
      "✅ Saved batch 187 (100 images)\n",
      "✅ Saved batch 188 (100 images)\n",
      "✅ Saved batch 189 (100 images)\n",
      "✅ Saved batch 190 (100 images)\n",
      "✅ Saved batch 191 (100 images)\n",
      "✅ Saved batch 192 (100 images)\n",
      "✅ Saved batch 193 (100 images)\n",
      "✅ Saved batch 194 (100 images)\n",
      "✅ Saved batch 195 (100 images)\n",
      "✅ Saved batch 196 (100 images)\n",
      "✅ Saved batch 197 (100 images)\n",
      "✅ Saved batch 198 (100 images)\n",
      "✅ Saved batch 199 (100 images)\n",
      "🎉 All images saved as .npy files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Set paths\n",
    "FRAMES_CSV = \"C:/Users/rushi/Desktop/Data set/frames_dataset.csv\"\n",
    "FRAMES_DIR = \"C:/Users/rushi/Desktop/Data set/frames\"\n",
    "IMG_SIZE = (64, 64)  # Resize images to reduce memory usage\n",
    "BATCH_SIZE = 1000  # Process images in batches\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(FRAMES_CSV)\n",
    "\n",
    "# Function to load images in batches\n",
    "def load_images(df, batch_size):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        frame_path = row['frame_path']\n",
    "        label = row['label']\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(frame_path)\n",
    "        \n",
    "        # Skip if image not found\n",
    "        if img is None:\n",
    "            continue  \n",
    "        \n",
    "        img = cv2.resize(img, IMG_SIZE)  # Resize to 64x64\n",
    "        img = img / 255.0  # Normalize (0-1)\n",
    "\n",
    "        X.append(img)\n",
    "        y.append(label)\n",
    "\n",
    "        # Process in batches\n",
    "        if len(X) >= batch_size:\n",
    "            yield np.array(X), np.array(y)\n",
    "            X, y = [], []  # Reset batch\n",
    "\n",
    "    # Yield remaining data\n",
    "    if X:\n",
    "        yield np.array(X), np.array(y)\n",
    "\n",
    "# Save images to NumPy arrays efficiently\n",
    "output_dir = \"C:/Users/rushi/Desktop/Data set/numpy_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "for i, (X_batch, y_batch) in enumerate(load_images(df, BATCH_SIZE)):\n",
    "    np.save(os.path.join(output_dir, f\"X_batch_{i}.npy\"), X_batch)\n",
    "    np.save(os.path.join(output_dir, f\"y_batch_{i}.npy\"), y_batch)\n",
    "    print(f\"✅ Saved batch {i} ({len(X_batch)} images)\")\n",
    "\n",
    "print(\"✅ All images processed and saved as .npy files!\")\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "# Set paths\n",
    "FRAMES_CSV = \"C:/Users/rushi/Desktop/Data set/frames_dataset.csv\"\n",
    "FRAMES_DIR = \"C:/Users/rushi/Desktop/Data set/frames\"\n",
    "OUTPUT_DIR = \"C:/Users/rushi/Desktop/Data set/numpy_data\"\n",
    "\n",
    "IMG_SIZE = (64, 64)  # Resize images\n",
    "BATCH_SIZE = 100  # Reduce batch size for memory efficiency\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(FRAMES_CSV)\n",
    "\n",
    "# Function to load & save images in smaller batches\n",
    "def save_images(df, batch_size):\n",
    "    X, y = [], []\n",
    "    batch_index = 0  # Track batch number\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        frame_path = row['frame_path']\n",
    "        label = row['label']\n",
    "\n",
    "        # Read image\n",
    "        img = cv2.imread(frame_path)\n",
    "\n",
    "        if img is None:\n",
    "            print(f\"❌ Skipping missing image: {frame_path}\")\n",
    "            continue  # Skip corrupt/missing images\n",
    "        \n",
    "        img = cv2.resize(img, IMG_SIZE)  # Resize\n",
    "        img = img / 255.0  # Normalize (0-1)\n",
    "\n",
    "        X.append(img)\n",
    "        y.append(label)\n",
    "\n",
    "        # Save in batches\n",
    "        if len(X) >= batch_size:\n",
    "            np.save(os.path.join(OUTPUT_DIR, f\"X_batch_{batch_index}.npy\"), np.array(X))\n",
    "            np.save(os.path.join(OUTPUT_DIR, f\"y_batch_{batch_index}.npy\"), np.array(y))\n",
    "            print(f\"✅ Saved batch {batch_index} ({len(X)} images)\")\n",
    "\n",
    "            # Clear memory\n",
    "            X, y = [], []\n",
    "            batch_index += 1  # Update batch number\n",
    "\n",
    "    # Save remaining images\n",
    "    if X:\n",
    "        np.save(os.path.join(OUTPUT_DIR, f\"X_batch_{batch_index}.npy\"), np.array(X))\n",
    "        np.save(os.path.join(OUTPUT_DIR, f\"y_batch_{batch_index}.npy\"), np.array(y))\n",
    "        print(f\"✅ Saved last batch {batch_index} ({len(X)} images)\")\n",
    "\n",
    "# Run the function\n",
    "save_images(df, BATCH_SIZE)\n",
    "\n",
    "print(\"🎉 All images saved as .npy files!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load .npy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20000 images and 20000 labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set paths for the .npy files\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "# Load the batches of images and labels\n",
    "for i in range(200):  # Update this to the number of batches you have\n",
    "    X_batch = np.load(f\"C:/Users/rushi/Desktop/Data set/numpy_data/X_batch_{i}.npy\")\n",
    "    y_batch = np.load(f\"C:/Users/rushi/Desktop/Data set/numpy_data/y_batch_{i}.npy\")\n",
    "    \n",
    "    X_data.append(X_batch)\n",
    "    y_data.append(y_batch)\n",
    "\n",
    "# Concatenate all batches\n",
    "X_data = np.concatenate(X_data, axis=0)\n",
    "y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "print(f\"Loaded {X_data.shape[0]} images and {y_data.shape[0]} labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (16000, 64, 64, 3), Testing data: (4000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Testing data: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
